{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[[3,4,5]], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findlat(city):\n",
    "    latitude=data.Lat[data['City']==city]\n",
    "    return latitude\n",
    "def findlong(city):\n",
    "    longitude=data.Long[data['City']==city]\n",
    "    return longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "   return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba42b456ba4c3da0bfe65daa3c6e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='city', index=60, options=('Abohar', 'Adilabad', 'Agartala', 'Agra'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "y = widgets.Dropdown(options=['Abohar',\n",
    " 'Adilabad',\n",
    " 'Agartala',\n",
    " 'Agra',\n",
    " 'Ahmadnagar',\n",
    " 'Ahmedabad',\n",
    " 'Aizawl  ',\n",
    " 'Ajmer',\n",
    " 'Akola',\n",
    " 'Alappuzha',\n",
    " 'Aligarh',\n",
    " 'Alipurduar',\n",
    " 'Allahabad',\n",
    " 'Alwar',\n",
    " 'Ambala',\n",
    " 'Amaravati',\n",
    " 'Amritsar',\n",
    " 'Asansol',\n",
    " 'Aurangabad',\n",
    " 'Aurangabad',\n",
    " 'Bakshpur',\n",
    " 'Bamanpuri',\n",
    " 'Baramula',\n",
    " 'Barddhaman',\n",
    " 'Bareilly',\n",
    " 'Belgaum',\n",
    " 'Bellary',\n",
    " 'Bengaluru',\n",
    " 'Bhagalpur',\n",
    " 'Bharatpur',\n",
    " 'Bharauri',\n",
    " 'Bhatpara',\n",
    " 'Bhavnagar',\n",
    " 'Bhilai',\n",
    " 'Bhilwara',\n",
    " 'Bhiwandi',\n",
    " 'Bhiwani',\n",
    " 'Bhopal ',\n",
    " 'Bhubaneshwar',\n",
    " 'Bhuj',\n",
    " 'Bhusaval',\n",
    " 'Bidar',\n",
    " 'Bijapur',\n",
    " 'Bikaner',\n",
    " 'Bilaspur',\n",
    " 'Brahmapur',\n",
    " 'Budaun',\n",
    " 'Bulandshahr',\n",
    " 'Calicut',\n",
    " 'Chanda',\n",
    " 'Chandigarh ',\n",
    " 'Chennai',\n",
    " 'Chikka Mandya',\n",
    " 'Chirala',\n",
    " 'Coimbatore',\n",
    " 'Cuddalore',\n",
    " 'Cuttack',\n",
    " 'Daman',\n",
    " 'Davangere',\n",
    " 'DehraDun',\n",
    " 'Delhi',\n",
    " 'Dhanbad',\n",
    " 'Dibrugarh',\n",
    " 'Dindigul',\n",
    " 'Dispur',\n",
    " 'Diu',\n",
    " 'Faridabad',\n",
    " 'Firozabad',\n",
    " 'Fyzabad',\n",
    " 'Gangtok',\n",
    " 'Gaya',\n",
    " 'Ghandinagar',\n",
    " 'Ghaziabad',\n",
    " 'Gopalpur',\n",
    " 'Gulbarga',\n",
    " 'Guntur',\n",
    " 'Gurugram',\n",
    " 'Guwahati',\n",
    " 'Gwalior',\n",
    " 'Haldia',\n",
    " 'Haora',\n",
    " 'Hapur',\n",
    " 'Haripur',\n",
    " 'Hata',\n",
    " 'Hindupur',\n",
    " 'Hisar',\n",
    " 'Hospet',\n",
    " 'Hubli',\n",
    " 'Hyderabad',\n",
    " 'Imphal',\n",
    " 'Indore',\n",
    " 'Itanagar',\n",
    " 'Jabalpur',\n",
    " 'Jaipur',\n",
    " 'Jammu',\n",
    " 'Jamshedpur',\n",
    " 'Jhansi',\n",
    " 'Jodhpur',\n",
    " 'Jorhat',\n",
    " 'Kagaznagar',\n",
    " 'Kakinada',\n",
    " 'Kalyan',\n",
    " 'Karimnagar',\n",
    " 'Karnal',\n",
    " 'Karur',\n",
    " 'Kavaratti',\n",
    " 'Khammam',\n",
    " 'Khanapur',\n",
    " 'Kochi',\n",
    " 'Kohima',\n",
    " 'Kolar',\n",
    " 'Kolhapur',\n",
    " 'Kolkata ',\n",
    " 'Kollam',\n",
    " 'Kota',\n",
    " 'Krishnanagar',\n",
    " 'Krishnapuram',\n",
    " 'Kumbakonam',\n",
    " 'Kurnool',\n",
    " 'Latur',\n",
    " 'Lucknow',\n",
    " 'Ludhiana',\n",
    " 'Machilipatnam',\n",
    " 'Madurai',\n",
    " 'Mahabubnagar',\n",
    " 'Malegaon Camp',\n",
    " 'Mangalore',\n",
    " 'Mathura',\n",
    " 'Meerut',\n",
    " 'Mirzapur',\n",
    " 'Moradabad',\n",
    " 'Mumbai',\n",
    " 'Muzaffarnagar',\n",
    " 'Muzaffarpur',\n",
    " 'Mysore',\n",
    " 'Nagercoil',\n",
    " 'Nalgonda',\n",
    " 'Nanded',\n",
    " 'Nandyal',\n",
    " 'Nasik',\n",
    " 'Navsari',\n",
    " 'Nellore',\n",
    " 'New Delhi',\n",
    " 'Nizamabad',\n",
    " 'Ongole',\n",
    " 'Pali',\n",
    " 'Panaji',\n",
    " 'Panchkula',\n",
    " 'Panipat',\n",
    " 'Parbhani',\n",
    " 'Pathankot',\n",
    " 'Patiala',\n",
    " 'Patna',\n",
    " 'Pilibhit',\n",
    " 'Porbandar',\n",
    " 'Port Blair',\n",
    " 'Proddatur',\n",
    " 'Puducherry',\n",
    " 'Pune',\n",
    " 'Puri',\n",
    " 'Purnea',\n",
    " 'Raichur',\n",
    " 'Raipur',\n",
    " 'Rajahmundry',\n",
    " 'Rajapalaiyam',\n",
    " 'Rajkot',\n",
    " 'Ramagundam',\n",
    " 'Rampura',\n",
    " 'Ranchi',\n",
    " 'Ratlam',\n",
    " 'Raurkela',\n",
    " 'Rohtak',\n",
    " 'Saharanpur',\n",
    " 'Saidapur',\n",
    " 'Saidpur',\n",
    " 'Salem',\n",
    " 'Samlaipadar',\n",
    " 'Sangli',\n",
    " 'Saugor',\n",
    " 'Shahbazpur',\n",
    " 'Shiliguri',\n",
    " 'Shillong ',\n",
    " 'Shimla',\n",
    " 'Shimoga',\n",
    " 'Sikar',\n",
    " 'Silchar',\n",
    " 'Silvassa',\n",
    " 'Sirsa',\n",
    " 'Sonipat',\n",
    " 'Srinagar',\n",
    " 'Surat',\n",
    " 'Tezpur',\n",
    " 'Thanjavur',\n",
    " 'Tharati Etawah',\n",
    " 'Thiruvananthapuram',\n",
    " 'Tiruchchirappalli',\n",
    " 'Tirunelveli',\n",
    " 'Tirupati',\n",
    " 'Tiruvannamalai',\n",
    " 'Tonk',\n",
    " 'Tuticorin',\n",
    " 'Udaipur',\n",
    " 'Ujjain',\n",
    " 'Vadodara',\n",
    " 'Valparai',\n",
    " 'Varanasi',\n",
    " 'Vellore',\n",
    " 'Vishakhapatnam',\n",
    " 'Vizianagaram',\n",
    " 'Warangal',\n",
    " 'Jorapokhar',\n",
    " 'Brajrajnagar',\n",
    " 'Talcher'],value='Delhi')\n",
    "\n",
    "@interact(city=y)\n",
    "def f(city):\n",
    "    latitude= findlat(city)\n",
    "    longitude = findlong(city)\n",
    "    db_tweets = pd.DataFrame(columns = ['username','location', 'text', 'hashtags'])\n",
    "    search_words = \"women\"\n",
    "    consumerKey = \"Ts7w6URgOYWXRfNuJwiUVoSz4\"\n",
    "    consumerSecretKey = \"iqItFdmQ84UtcMT7PDIx9Ygovjzd6PW4akEI27Q6MUmqIrbjaR\"\n",
    "    accessToken = \"1171293910175383554-nSYc45W3XCEOJTc1r89uPI0m2I3bON\"\n",
    "    accessTokenSecret = \"sVnWSBw6EdVXq0jZC3xRvBXtdSGZObRRfyF1mdOk953jg\"\n",
    "    \n",
    "    authenticate = tweepy.OAuthHandler(consumerKey, consumerSecretKey) \n",
    "        \n",
    "    # Set the access token and access token secret\n",
    "    authenticate.set_access_token(accessToken, accessTokenSecret) \n",
    "        \n",
    "    # Creating the API object while passing in auth information\n",
    "    api = tweepy.API(authenticate, wait_on_rate_limit = True)\n",
    "    db_tweets = pd.DataFrame(columns = ['username','location', 'text', 'hashtags'])\n",
    "    for i in range(0, 1):\n",
    "        tweets = tweepy.Cursor(api.search, lang=\"en\",q=search_words, geocode=\"%f,%f,%dkm\" % (latitude, longitude, 100), tweet_mode='extended').items(100)\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "    for tweet in tweet_list:\n",
    "        username = tweet.user.screen_name\n",
    "        location = tweet.user.location\n",
    "        hashtags = tweet.entities['hashtags']\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "        except:  # Not a Retweet\n",
    "            text = tweet.full_text\n",
    "            ith_tweet = [username, location, text, hashtags]\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "    dataset =db_tweets\n",
    "    dataset['text'].isna().sum() \n",
    "    dataset['clean_tweet'] = dataset['text'].apply(lambda x: ' '.join([tweet for tweet in x.split() if not tweet.startswith(\"@\")]))\n",
    "    dataset['clean_tweet'] = dataset['clean_tweet'].apply(lambda x: ' '.join([tweet for tweet in x.split() if not tweet.isnumeric()]))\n",
    "    slang = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n",
    "              'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother', 'cud':'could',\n",
    "             'fud':'food', 'u':'you', 'ur':'your', 'frm': 'from'}\n",
    "    dataset['clean_tweet'] = dataset['clean_tweet'].apply(lambda x : ' '.join(slang[word] if word in slang else word for word in x.split()))\n",
    "    dataset['Hashtags'] = dataset['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))\n",
    "    dataset.drop('text',axis=1,inplace=True)\n",
    "    #dataset.drop('hashtags',axis=1,inplace=True)\n",
    "    dataset['clean_tweet'] = dataset['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word in set(stopwords.words('english'))]))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset['clean_tweet'] = dataset['clean_tweet'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    ps = PorterStemmer()\n",
    "    dataset['clean_tweet'] = dataset['clean_tweet'].apply(lambda x : ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    dataset['text']=dataset['clean_tweet']\n",
    "    dataset.drop('clean_tweet',axis=1,inplace=True)\n",
    "    dataset['Subjectivity'] = dataset['text'].apply(getSubjectivity)\n",
    "    dataset['Polarity'] = dataset['text'].apply(getPolarity)\n",
    "    dataset['Analysis'] = dataset['Polarity'].apply(getAnalysis)\n",
    "    ntweets = dataset[dataset.Analysis == 'Negative']\n",
    "    ntext = ntweets['text']\n",
    "    nusers=ntweets['username'].tolist()\n",
    "    print(nusers)\n",
    "    df=dataset\n",
    "    allWords = ' '.join([twts for twts in df['text']])\n",
    "    new_stopwords=[\"woman\",\"women\",\"girl\",\"women'\",\"https\"]\n",
    "    wc = WordCloud(width = 800, height = 500, max_font_size = 110, max_words=100, stopwords=new_stopwords).generate(allWords)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.title('Sentiment Analysis')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Counts')\n",
    "    df['Analysis'].value_counts().plot(kind = 'bar')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
